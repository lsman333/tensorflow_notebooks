{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOxHEpet2QUkoc9t+U1Y0Kt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c1f39d8133f34b3a96faa3c725e6b841":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30dbc51c0fb74e0080ba7209c7067e50","IPY_MODEL_6df55f11ebdc45d7a1f16efe238619d3","IPY_MODEL_0e3fdfd723f545cd8c58d1170bacb01b"],"layout":"IPY_MODEL_708a9d038f6e4c9c8dcea686297698e0"}},"30dbc51c0fb74e0080ba7209c7067e50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bacc6d14e6534c94b6b10e5aed66d676","placeholder":"​","style":"IPY_MODEL_00efc0e3b83e4dd59686e2aec83be92e","value":"Dl Completed...: 100%"}},"6df55f11ebdc45d7a1f16efe238619d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_953ebca8cc3b45339cba8132a0051398","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d2db7b9eadcd428d8b56cc3854df18ab","value":1}},"0e3fdfd723f545cd8c58d1170bacb01b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8d05f8980e44fc09d6baa4d8d43c4f7","placeholder":"​","style":"IPY_MODEL_b86a970cec8c42dda9005edcd14144cb","value":" 1/1 [09:13&lt;00:00, 339.16s/ url]"}},"708a9d038f6e4c9c8dcea686297698e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bacc6d14e6534c94b6b10e5aed66d676":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00efc0e3b83e4dd59686e2aec83be92e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"953ebca8cc3b45339cba8132a0051398":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d2db7b9eadcd428d8b56cc3854df18ab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8d05f8980e44fc09d6baa4d8d43c4f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b86a970cec8c42dda9005edcd14144cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fe27df48d9649e4819ed6bb424eeaf5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_247e74e461ed4508a237bf4367cd4e57","IPY_MODEL_b9b4315ab2c34c2dafc1677a902088c9","IPY_MODEL_45cacf3fe884433e9d9feb726b34a414"],"layout":"IPY_MODEL_39a87a84618b4e9dad358bc4665a03da"}},"247e74e461ed4508a237bf4367cd4e57":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_79a43fa284534c75adff36c5cebe29a1","placeholder":"​","style":"IPY_MODEL_3acdff6ab1094aecbaa18560b9101add","value":"Dl Size...: 100%"}},"b9b4315ab2c34c2dafc1677a902088c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ac799c2ec714ae0b686b53b5fc076ac","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6b52667737b47c6b241b6bbabbb207c","value":1}},"45cacf3fe884433e9d9feb726b34a414":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_087d41026d8347bdbce248a69a9c3c75","placeholder":"​","style":"IPY_MODEL_00d52a9d9f194a00bab0a30cab92a7ad","value":" 4764/4764 [09:13&lt;00:00, 15.00 MiB/s]"}},"39a87a84618b4e9dad358bc4665a03da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79a43fa284534c75adff36c5cebe29a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3acdff6ab1094aecbaa18560b9101add":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ac799c2ec714ae0b686b53b5fc076ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a6b52667737b47c6b241b6bbabbb207c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"087d41026d8347bdbce248a69a9c3c75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00d52a9d9f194a00bab0a30cab92a7ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1cafb9ea3b174241be3985f18429e227":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fed52a24e5db48d6877e833656de1e04","IPY_MODEL_aae7f668b25f46bea60d08f4a6f43b00","IPY_MODEL_711f3e2280ac4be9bbac868992e4f414"],"layout":"IPY_MODEL_2f5bfd8bcaee4d709c9318393d273ee9"}},"fed52a24e5db48d6877e833656de1e04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b8d07dd86584054a147eafc23274b9e","placeholder":"​","style":"IPY_MODEL_91994e95480a41bfa8556eedcdd5bb08","value":"Extraction completed...:   0%"}},"aae7f668b25f46bea60d08f4a6f43b00":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d3e0ba1aae44b8f9fd04b4f661fc1a0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e4d53f050d40447f98872f4f11b410e0","value":0}},"711f3e2280ac4be9bbac868992e4f414":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90e94fd6a75445c8b7e509328b575bcf","placeholder":"​","style":"IPY_MODEL_0eadcf30fd744ed4960815270657562c","value":" 0/87804 [09:13&lt;?, ? file/s]"}},"2f5bfd8bcaee4d709c9318393d273ee9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b8d07dd86584054a147eafc23274b9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91994e95480a41bfa8556eedcdd5bb08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d3e0ba1aae44b8f9fd04b4f661fc1a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e4d53f050d40447f98872f4f11b410e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"90e94fd6a75445c8b7e509328b575bcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eadcf30fd744ed4960815270657562c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["#We need to install tf version 2.15 for compatibility with this notebook (written in March 2024).\n","!pip uninstall tensorflow\n","!pip install tensorflow==2.15.0\n","!tensorflow --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ZhCD3gLE0K7","executionInfo":{"status":"ok","timestamp":1731041018736,"user_tz":300,"elapsed":70933,"user":{"displayName":"Mark","userId":"05293542057377705486"}},"outputId":"33d746ec-c04a-482d-bc48-08e0c0a18429"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.17.0\n","Uninstalling tensorflow-2.17.0:\n","  Would remove:\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.10/dist-packages/tensorflow-2.17.0.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n","Proceed (Y/n)? y\n","  Successfully uninstalled tensorflow-2.17.0\n","Collecting tensorflow==2.15.0\n","  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n","Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n","  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.12.2)\n","Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n","  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.1)\n","Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n","  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n","Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n","  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.6)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n","Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, ml-dtypes, keras, tensorboard, tensorflow\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.16.0\n","    Uninstalling wrapt-1.16.0:\n","      Successfully uninstalled wrapt-1.16.0\n","  Attempting uninstall: ml-dtypes\n","    Found existing installation: ml-dtypes 0.4.1\n","    Uninstalling ml-dtypes-0.4.1:\n","      Successfully uninstalled ml-dtypes-0.4.1\n","  Attempting uninstall: keras\n","    Found existing installation: keras 3.4.1\n","    Uninstalling keras-3.4.1:\n","      Successfully uninstalled keras-3.4.1\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.17.0\n","    Uninstalling tensorboard-2.17.0:\n","      Successfully uninstalled tensorboard-2.17.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorstore 0.1.67 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n","/bin/bash: line 1: tensorflow: command not found\n"]}]},{"cell_type":"markdown","source":["# Milestone Project 1: Food Vision Big\n","See an annotated version of this notebook on github\n"],"metadata":{"id":"kZYWIExCsty_"}},{"cell_type":"markdown","source":["## Check GPU\n","\n","Google Colab offers free GPUs.  However not all are compatible with mixed precision training.  Google Colab offers:\n","* K80\n","* P100\n","* T4 (only one compatible with mixed precision training)\n","\n","Knowing this, in order to use mixed precision training, we need access to a Tesla T4 from Google Colab or if using our own hardware, our GPU needs a score of 7.0+"],"metadata":{"id":"xwqPid8ztDLo"}},{"cell_type":"code","source":["!nvidia-smi -L"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pLKTCGbtEby","executionInfo":{"status":"ok","timestamp":1731041151022,"user_tz":300,"elapsed":393,"user":{"displayName":"Mark","userId":"05293542057377705486"}},"outputId":"a6f87fee-674a-4240-c786-22dd80d3054e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-a2239727-7ae8-ab64-5083-67e706fd1855)\n"]}]},{"cell_type":"markdown","source":["## Get helper functions\n","\n","In past modules we have created a bunch of helper functions to do small tasks required for our notebooks.  Rather than rewrite all of these, we can import a script and load them in from there.  The script we've got available can be found on GitHub:https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n"],"metadata":{"id":"yQTZTrlFvz7P"}},{"cell_type":"code","source":["# Download helper functions script\n","!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XVNzTJ6jwSzU","executionInfo":{"status":"ok","timestamp":1731041151649,"user_tz":300,"elapsed":312,"user":{"displayName":"Mark","userId":"05293542057377705486"}},"outputId":"df5e5eeb-cc01-470b-9e74-a6e665963427"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-11-08 04:45:50--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10246 (10K) [text/plain]\n","Saving to: ‘helper_functions.py’\n","\n","helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n","\n","2024-11-08 04:45:51 (22.5 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n","\n"]}]},{"cell_type":"code","source":["# Import series of helper functions for the notebook\n","from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"],"metadata":{"id":"0TspQXPRwlCO","executionInfo":{"status":"ok","timestamp":1731041158063,"user_tz":300,"elapsed":6415,"user":{"displayName":"Mark","userId":"05293542057377705486"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Use TensorFlow datasets to download data\n","\n","If you want to get an overview of tensorflow datasets (TFDS) read the guide"],"metadata":{"id":"dkg4Bc1cxTVu"}},{"cell_type":"code","source":["# Get TensorFlow datasets\n","import tensorflow_datasets as tfds"],"metadata":{"id":"6ArSlvFRyFbK","executionInfo":{"status":"ok","timestamp":1731041159108,"user_tz":300,"elapsed":1048,"user":{"displayName":"Mark","userId":"05293542057377705486"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# List all of the available datasets in tensorflow\n","datasets_list = tfds.list_builders() # Get all the available datasets in tfds\n","print(\"food101\" in datasets_list) # is our target dataset in the list of TFDS datasets?"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uP8TchA_yLoO","executionInfo":{"status":"ok","timestamp":1731041168824,"user_tz":300,"elapsed":9717,"user":{"displayName":"Mark","userId":"05293542057377705486"}},"outputId":"6092c1fd-2484-406d-d506-3ed0a96b3942"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["# Load in the data (takes 5-6 minutes in google colab)\n","(train_data, test_data), ds_info = tfds.load(name = \"food101\",\n","                                             split = [\"train\", \"validation\"],\n","                                             shuffle_files = True,\n","                                             as_supervised = True, # data gets returned in tuple format (data, label)\n","                                             with_info = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130,"referenced_widgets":["c1f39d8133f34b3a96faa3c725e6b841","30dbc51c0fb74e0080ba7209c7067e50","6df55f11ebdc45d7a1f16efe238619d3","0e3fdfd723f545cd8c58d1170bacb01b","708a9d038f6e4c9c8dcea686297698e0","bacc6d14e6534c94b6b10e5aed66d676","00efc0e3b83e4dd59686e2aec83be92e","953ebca8cc3b45339cba8132a0051398","d2db7b9eadcd428d8b56cc3854df18ab","e8d05f8980e44fc09d6baa4d8d43c4f7","b86a970cec8c42dda9005edcd14144cb","0fe27df48d9649e4819ed6bb424eeaf5","247e74e461ed4508a237bf4367cd4e57","b9b4315ab2c34c2dafc1677a902088c9","45cacf3fe884433e9d9feb726b34a414","39a87a84618b4e9dad358bc4665a03da","79a43fa284534c75adff36c5cebe29a1","3acdff6ab1094aecbaa18560b9101add","6ac799c2ec714ae0b686b53b5fc076ac","a6b52667737b47c6b241b6bbabbb207c","087d41026d8347bdbce248a69a9c3c75","00d52a9d9f194a00bab0a30cab92a7ad","1cafb9ea3b174241be3985f18429e227","fed52a24e5db48d6877e833656de1e04","aae7f668b25f46bea60d08f4a6f43b00","711f3e2280ac4be9bbac868992e4f414","2f5bfd8bcaee4d709c9318393d273ee9","7b8d07dd86584054a147eafc23274b9e","91994e95480a41bfa8556eedcdd5bb08","1d3e0ba1aae44b8f9fd04b4f661fc1a0","e4d53f050d40447f98872f4f11b410e0","90e94fd6a75445c8b7e509328b575bcf","0eadcf30fd744ed4960815270657562c"]},"id":"h6ijb0ytyx8l","outputId":"eeaa14ce-c4ff-43ed-8993-5a64c8692471"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset 4.65 GiB (download: 4.65 GiB, generated: Unknown size, total: 4.65 GiB) to /root/tensorflow_datasets/food101/2.0.0...\n"]},{"output_type":"display_data","data":{"text/plain":["Dl Completed...: 0 url [00:00, ? url/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f39d8133f34b3a96faa3c725e6b841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Dl Size...: 0 MiB [00:00, ? MiB/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fe27df48d9649e4819ed6bb424eeaf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extraction completed...: 0 file [00:00, ? file/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cafb9ea3b174241be3985f18429e227"}},"metadata":{}}]},{"cell_type":"code","source":["# Features of Food101 from TFDS\n","ds_info.features"],"metadata":{"id":"IlHgk2Kfz9Yh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the class names\n","class_names = ds_info.features[\"label\"].names\n","class_names"],"metadata":{"id":"TK0JR99x75vA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exploring the Food101 data from tenslorflow datasets\n","To become one with our data, we want to find:\n","* Class names\n","* The shape of our input data (image tensors)\n","* The datatype of our input data\n","* What the labels look like (ie are they one-hot encoded or label-encoded?)\n","* Do the labels match up with the class names?"],"metadata":{"id":"9R5OP2209EQD"}},{"cell_type":"code","source":["# Take one sample of the train_data\n","train_one_sample = train_data.take(1)   #samples are in format (image_tensor, label)"],"metadata":{"id":"ezbI5f3L9ntU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# What does one sample of our training data look like?\n","train_one_sample"],"metadata":{"id":"LMRHSe6h-EDp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# output info about our training sample\n","for image,label in train_one_sample:\n","  print(f'''\n","  Image shape: {image.shape},\n","  Image datatype: {image.dtype},\n","  Target class from Food101 (tensor form): {label},\n","  Class name (string form): {class_names[label.numpy()]}\n","  ''')"],"metadata":{"id":"hKpG4SrR-PzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# What does our image tensor from TFDS's Food101 look like?\n","image"],"metadata":{"id":"5cCeq2jT_fiE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# What are the min and max values of our image tensor?\n","import tensorflow as tf\n","tf.reduce_min(image), tf.reduce_max(image)"],"metadata":{"id":"l4cB_yTA_vqI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot an image from tensorflow"],"metadata":{"id":"hiQL_owuACJ8"}},{"cell_type":"code","source":["# Plot an image tensor\n","import matplotlib.pyplot as plt\n","plt.imshow(image)\n","plt.title(class_names[label.numpy()]) # Add title to image to verify the label is associated with the right image\n","plt.axis(False)"],"metadata":{"id":"1ynelw2UATER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalize the tensor values and the image sizes\n"],"metadata":{"id":"X6H31HW3CTPa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Create preprocessing functions for our data\n","\n","Neural networks perform best when data is in a certain way (eg, batched, normalized, etc.)\n","\n","However, not all data, including data from tensorflow datasets, comes like this.  So in order to get it ready for a neural network, you'll often have to write preprocessing functions and map it to your data.\n","\n","What we know about our data:\n","* It's in uint8 datatype\n","* It's comprised of all different sized tensors / different sized images\n","* Not scaled (the pixel values are between 0 and 255)\n","\n","What we know models like:\n","* Data in `float32` dtype (or for mixed precision `float16` and `float32` dtype)\n","* For batches, tensorflow likes all of the tensors within a batch to be of the same size\n","* Scaled (Values between 0 and 1) - also called normalized tensors generally perform better\n","\n","With these things in mind, we've got a few things we can tackle with a preprocessing function.\n","\n","Since we're going to be using an EfficientNetBX pretrained model from tf.keras.applications, we don't need to rescale our data.  These architectures have rescaling built in.\n","\n","This means our functions need to:\n","1. Reshape our images to all the same size\n","2. Convert the dtype of our image tensors from uint8 to float32\n"],"metadata":{"id":"t20nmZ0uDBfN"}},{"cell_type":"code","source":["# Make a function for preprocessing images\n","def preprocess_img(image, label, img_shape=224):\n","  '''\n","  Converts image datatype from uint8 -> float32 and reshapes image\n","  to [img_shape, img_shape, colour_channels]\n","  '''\n","  image=tf.image.resize(image, [img_shape, img_shape])  #reshape target image\n","  return tf.cast(image, tf.float32), label # return (float32_image, label) tuple\n","\n","# Preprocess a single sample image and check the outputs\n","preprocessed_img = preprocess_img(image, label)[0]\n","print(f\"Image before preprocessing:\\n{image[:1]}..., \\nShape: {image.shape}, \\nDatatype: {image.dtype}\\n \")\n","print(f\"Image after processing:\\n{preprocessed_img[:1]}... \\nShape: {preprocessed_img.shape}, \\nDatatype: {preprocessed_img.dtype}\\n \")\n","len(image)\n","len(preprocessed_img)"],"metadata":{"id":"UswjE4eIVOIj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Batch and prepare datasets\n","\n","We're now going to make our data input pipeline run really fast.\n","\n","For more resources on this, i'd highly recommend going through the following guide. https://www.tensorflow.org/guide/data_performance"],"metadata":{"id":"h2Xi3M9OTSMu"}},{"cell_type":"code","source":["# Map preprocessing function to training data (and parallelize)\n","train_data = train_data.map(map_func=preprocess_img, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","#Shuffle train_data and turn it into batches and prefetch it (load it faster)\n","train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","# Map preprocessing function to test data\n","test_data = test_data.map(preprocess_img, num_parallel_calls=tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)"],"metadata":{"id":"jZSyuGGjT9xS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, test_data"],"metadata":{"id":"k8YNgaRcZxge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> \"Hey TensorFlow, map this preprocessing function (`preprocess_img`) across our training dataset, then shuffle the number of elements and then batch them together and finally make sure you prepare new batches (prefetch) while the model is looking through finding patterns in the current batch\""],"metadata":{"id":"w-RPKuuma0Dn"}},{"cell_type":"markdown","source":["## Create modelling callbacks\n","\n","We're going to create a couple of callbacks to help us while our model trains.  Specifically:\n","1. Tensorboard callback (log training results so we can visualize them later if need be)\n","2. Model checkpoint callback (save our model's progress after feature extraction)\n","\n"],"metadata":{"id":"czfGipMizylt"}},{"cell_type":"code","source":["# Create tensorboard callback (import from helper_functions.py)\n","from helper_functions import create_tensorboard_callback\n","\n","# Create a model checkpoint callback to save a model's progress during training\n","checkpoint_path = \"model_checkpoints/cp.ckpt\"\n","model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n","                                                      monitor=\"val_acc\",\n","                                                      save_best_only=True,\n","                                                      verbose=0)  # don't print whether or not model is being saved"],"metadata":{"id":"G-oPRj3W0vV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.__version__"],"metadata":{"id":"ghq3_dkw4Ds0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Setup mixed precision training\n","\n","First and foremost, for a deeper understanding of mixed precision training, check out the TensorFlow guide for mixed precision: https://www.tensorflow.org/guide/mixed_precision\n","\n","Mixed precision uses a combination of float32 and float16 datatypes to speed up model performance"],"metadata":{"id":"2sgpA5SP4urk"}},{"cell_type":"code","source":["# Turn on mixed precision training\n","from tensorflow.keras import mixed_precision\n","mixed_precision.set_global_policy(\"mixed_float16\")    #set global data policy to mixed precision"],"metadata":{"id":"sBl8enWb6JA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mixed_precision.global_policy()"],"metadata":{"id":"Nn_Xtu9k8BbN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Build feature extraction model\n","\n"],"metadata":{"id":"rMujxZLrBphw"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","from tensorflow.keras.layers.experimental import preprocessing\n","\n","# Create base model\n","input_shape = (224,224,3)\n","base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n","base_model.trainable = False\n","\n","# Create functional model\n","inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n","# Note: efficientNetBX models have rescaling built-in, but if your model doesn't, you can have a layer like below:\n","# x = preprocessing.rescaling(1./255)(x)\n","x = base_model(inputs, training=False) # Make sure layers which should be in inference only mode stay like that\n","x = layers.GlobalAveragePooling2D()(x)\n","x = layers.Dense(len(class_names))(x)\n","outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x)\n","model = tf.keras.Model(inputs,outputs)\n","\n","#Compile your model\n","model.compile(loss=\"sparse_categorical_crossentropy\",\n","              optimizer=tf.keras.optimizers.Adam(),\n","              metrics=\"accuracy\")\n","\n"],"metadata":{"id":"0HwRBHDyBthu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"j3KdYpf8FFDc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for layer in model.layers:\n","  print(f\"name: {layer.name}, trainable: {model.trainable}, dtype: {layer.dtype}, dtype_policy: {layer.dtype_policy}\")"],"metadata":{"id":"lFFuiYbZFGi6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Going through the above, we can see that:\n","* `Layer.name`: the human-readable name of the layer\n","* `layer.trainable`: is the layer trainable?\n","* `layer.dtype`: the data type of the stored variable\n","* `layer.dtype_policy`: the data type policy a layer computes on its variables with"],"metadata":{"id":"DVy8f_ZIHIw1"}},{"cell_type":"code","source":["# Check the dtype policy of the efficientnetb0 layer\n","for layer_number, layer in enumerate(model.layers[1].layers):\n","  print(f\"{layer_number} name: {layer.name}, trainable: {model.trainable}, dtype: {layer.dtype}, dtype_policy: {layer.dtype_policy}\")"],"metadata":{"id":"yQWPQFxQIDNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit the model on the training data with 3 epochs\n","history_101_food_classes_feature_extract = model.fit(train_data,\n","                                                     steps_per_epoch = len(train_data),\n","                                                     validation_steps = (int(0.15 * len(test_data))),\n","                                                      validation_data = test_data,\n","                                                      epochs=3,\n","                                                      callbacks=[model_checkpoint])"],"metadata":{"id":"3CeDOUxqKv1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on whole test data set\n","results_feature_extract_model = model.evaluate(test_data)\n","results_feature_extract_model"],"metadata":{"id":"f-cs3kG8Qbff"},"execution_count":null,"outputs":[]}]}